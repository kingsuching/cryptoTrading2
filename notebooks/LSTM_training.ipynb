{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lstm-cell-01",
   "metadata": {},
   "outputs": [],
   "source": "import os, sys\nfrom pathlib import Path\n\n_root = next((p for p in [Path(os.getcwd()), *Path(os.getcwd()).parents]\n              if p.name == 'cryptoTrading2'), None)\nif _root:\n    os.chdir(str(_root))\nif os.getcwd() not in sys.path:\n    sys.path.insert(0, os.getcwd())\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom files.functions import (\n    base_dir,\n    fullDataPath,\n    dataSetup,\n    trainingCols,\n    normalize_data,\n    create_sequences,\n    _get_device,\n    _lstm_param_grid,\n    _torch_train,\n    _seq_future_forecast_torch,\n    _standardized_rmse,\n    _save_model_artifact,\n    _save_validation_predictions,\n    _save_future_predictions,\n    _save_metrics,\n)\nfrom files.CONSTANTS import COIN, RESPONSE_VARIABLE, TRAINING_COLUMNS, TEST_DAYS, TRAIN_PCT\nfrom implementations.lstm_model import LSTMModel"
  },
  {
   "cell_type": "markdown",
   "id": "lstm-cell-02",
   "metadata": {},
   "source": [
    "# LSTM Training\n",
    "\n",
    "Multi-layer Long Short-Term Memory (LSTM) model using PyTorch for sequential crypto price forecasting.\n",
    "This notebook normalizes the full multivariate feature set, creates sliding-window sequences,\n",
    "performs hyperparameter tuning (hidden size, layers, dropout, sequence length, scaler method)\n",
    "with early stopping, and produces both validation and future price predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lstm-cell-03",
   "metadata": {},
   "outputs": [],
   "source": "# ── Step 1: Load and Prepare Data ────────────────────────────────────────────\nraw_path = fullDataPath(COIN)\ndata = pd.read_csv(raw_path)\ndaily_data = dataSetup(data, trainingColPath=TRAINING_COLUMNS, response=RESPONSE_VARIABLE)\ncols = trainingCols(TRAINING_COLUMNS)\ndata_full = daily_data[cols + [RESPONSE_VARIABLE]].copy()\nprint(f\"Dataset shape: {daily_data.shape}\")\nprint(f\"Features ({len(cols)}): {cols}\")\nprint(f\"Date range: {daily_data.index[0]} → {daily_data.index[-1]}\")\nprint(f\"\\ndata_full shape: {data_full.shape}  (features + target)\")\n\ndaily_data[[RESPONSE_VARIABLE]].plot(title=f\"{COIN} Close Price\", figsize=(12, 4))\nplt.tight_layout()\nplt.show()\n\ndevice = _get_device()\nprint(f\"\\nUsing device: {device}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lstm-cell-04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Step 3: Define Hyperparameter Grid ───────────────────────────────────────\n",
    "param_grid = _lstm_param_grid()\n",
    "scaler_methods = ['minmax', 'standard']\n",
    "\n",
    "print(f\"LSTM param grid size: {len(param_grid)} configs\")\n",
    "print(f\"Scaler methods: {scaler_methods}\")\n",
    "print(f\"Total combinations: {len(param_grid) * len(scaler_methods)}\")\n",
    "print(\"\\nSample configs:\")\n",
    "for cfg in param_grid[:3]:\n",
    "    print(\" \", cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lstm-cell-05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Step 4: Tune Hyperparameters ─────────────────────────────────────────────\n",
    "# Tune over scaler methods, sequence lengths, and LSTM architecture hyperparameters\n",
    "best_score = np.inf\n",
    "best_combo = None\n",
    "tuning_results = []\n",
    "\n",
    "for sm in scaler_methods:\n",
    "    scaled_data, feat_sc, tgt_sc = normalize_data(data_full, method=sm, target_col=RESPONSE_VARIABLE)\n",
    "    for params in param_grid:\n",
    "        seq_len = params['seq_len']\n",
    "        X_seq, y_seq = create_sequences(scaled_data, sequence_length=seq_len, prediction_horizon=1)\n",
    "        if len(X_seq) < 10:\n",
    "            continue\n",
    "        n_train = max(1, int(len(X_seq) * TRAIN_PCT))\n",
    "        X_tr = torch.tensor(X_seq[:n_train], dtype=torch.float32)\n",
    "        y_tr = torch.tensor(y_seq[:n_train].flatten(), dtype=torch.float32)\n",
    "        X_va = torch.tensor(X_seq[n_train:], dtype=torch.float32)\n",
    "        y_va = torch.tensor(y_seq[n_train:].flatten(), dtype=torch.float32)\n",
    "        if len(X_va) == 0:\n",
    "            continue\n",
    "        hp = {k: v for k, v in params.items() if k != 'seq_len'}\n",
    "        m = LSTMModel(input_size=X_tr.shape[2], **hp)\n",
    "        m, val_loss = _torch_train(m, X_tr, y_tr, X_va, y_va,\n",
    "                                   lr=1e-3, epochs=30, batch_size=32,\n",
    "                                   patience=5, device=device)\n",
    "        rec = {**params, 'scaler': sm, 'val_loss': val_loss}\n",
    "        tuning_results.append(rec)\n",
    "        if val_loss < best_score:\n",
    "            best_score = val_loss\n",
    "            best_combo = rec.copy()\n",
    "        print(f\"  scaler={sm} seq={seq_len} hidden={params['hidden_size']} \"\n",
    "              f\"layers={params['num_layers']} dropout={params['dropout']}  \"\n",
    "              f\"val_loss={val_loss:.6f}\")\n",
    "\n",
    "if best_combo is None:\n",
    "    best_combo = {'hidden_size': 64, 'num_layers': 2, 'dropout': 0.1,\n",
    "                  'seq_len': 30, 'scaler': 'minmax', 'val_loss': np.nan}\n",
    "\n",
    "print(f\"\\nBest combo: {best_combo}\")\n",
    "\n",
    "tuning_df = pd.DataFrame(tuning_results).sort_values('val_loss').reset_index(drop=True)\n",
    "print(\"\\nTop-5 tuning results:\")\n",
    "print(tuning_df.head(5).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lstm-cell-06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Step 5: Train Best Model ──────────────────────────────────────────────────\n",
    "best_sm = best_combo['scaler']\n",
    "SEQ_LEN = best_combo['seq_len']\n",
    "best_hp = {k: v for k, v in best_combo.items()\n",
    "           if k not in ('seq_len', 'scaler', 'val_loss')}\n",
    "\n",
    "# Normalize with best scaler on full data\n",
    "scaled_data, feat_sc, tgt_sc = normalize_data(data_full, method=best_sm, target_col=RESPONSE_VARIABLE)\n",
    "\n",
    "# ── Step 2 (Train/Val Split) embedded here for final training ─────────────────\n",
    "X_seq, y_seq = create_sequences(scaled_data, sequence_length=SEQ_LEN, prediction_horizon=1)\n",
    "n_train = max(1, int(len(X_seq) * TRAIN_PCT))\n",
    "X_tr = torch.tensor(X_seq[:n_train], dtype=torch.float32)\n",
    "y_tr = torch.tensor(y_seq[:n_train].flatten(), dtype=torch.float32)\n",
    "X_va = torch.tensor(X_seq[n_train:], dtype=torch.float32)\n",
    "y_va_np = y_seq[n_train:].flatten()\n",
    "y_va_t = torch.tensor(y_va_np, dtype=torch.float32)\n",
    "\n",
    "print(f\"Sequence length: {SEQ_LEN}\")\n",
    "print(f\"Train sequences: {len(X_tr)}  |  Val sequences: {len(X_va)}\")\n",
    "print(f\"Input size (num features): {X_tr.shape[2]}\")\n",
    "print(f\"Scaler: {best_sm}\")\n",
    "print(f\"LSTM hyperparams: {best_hp}\")\n",
    "\n",
    "best_model = LSTMModel(input_size=X_tr.shape[2], **best_hp)\n",
    "best_model, final_val_loss = _torch_train(\n",
    "    best_model, X_tr, y_tr, X_va, y_va_t,\n",
    "    lr=1e-3, epochs=50, batch_size=32, patience=10, device=device\n",
    ")\n",
    "best_model.eval()\n",
    "print(f\"\\nFinal val loss: {final_val_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lstm-cell-07",
   "metadata": {},
   "outputs": [],
   "source": "# ── Step 6: Save Model to models/{COIN}/ ─────────────────────────────────────\n_models_dir = os.path.join(base_dir(), 'models', COIN)\nos.makedirs(_models_dir, exist_ok=True)\nmodel_pt_path = os.path.join(_models_dir, f'{COIN}_lstm_model.pt')\ntorch.save(best_model.state_dict(), model_pt_path)\nprint(f\"Model state dict saved to: {model_pt_path}\")\n\nmeta_path = _save_model_artifact(\n    {'feat_scaler': feat_sc, 'tgt_scaler': tgt_sc, 'params': best_combo,\n     'seq_len': SEQ_LEN, 'scaler_method': best_sm},\n    COIN,\n    f'{COIN}_lstm_meta.pkl'\n)\nprint(f\"Meta artifact saved to: {meta_path}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lstm-cell-08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Step 7: Predict on Validation Set ────────────────────────────────────────\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    val_preds_scaled = best_model(X_va).squeeze(-1).numpy()\n",
    "\n",
    "# Inverse-transform predictions and ground truth back to price scale\n",
    "val_preds = tgt_sc.inverse_transform(val_preds_scaled.reshape(-1, 1)).flatten()\n",
    "val_true = tgt_sc.inverse_transform(y_va_np.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Align index with daily_data\n",
    "val_idx = daily_data.index[n_train + SEQ_LEN: n_train + SEQ_LEN + len(val_preds)]\n",
    "val_df = pd.DataFrame({'predicted_price': val_preds}, index=val_idx[:len(val_preds)])\n",
    "\n",
    "# Compute RMSE\n",
    "rmse = float(np.sqrt(np.mean((val_true - val_preds) ** 2)))\n",
    "std_rmse = rmse / float(np.std(val_true)) if float(np.std(val_true)) > 0 else rmse\n",
    "print(f\"Validation RMSE:            {rmse:,.2f}\")\n",
    "print(f\"Standardized RMSE (÷ std):  {std_rmse:.4f}\")\n",
    "\n",
    "# Plot actual vs predicted\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "ax.plot(val_idx[:len(val_true)], val_true, label='Actual', linewidth=2)\n",
    "ax.plot(val_df.index, val_df['predicted_price'], label='Predicted (LSTM)', linestyle='--')\n",
    "ax.set_title(f\"{COIN} LSTM — Validation Set: Actual vs Predicted\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Price (USD)\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lstm-cell-09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Step 8: Predict Next TEST_DAYS Days ──────────────────────────────────────\n",
    "future_df = _seq_future_forecast_torch(\n",
    "    best_model, scaled_data, SEQ_LEN, tgt_sc, daily_data, n=TEST_DAYS\n",
    ")\n",
    "\n",
    "print(f\"Future predictions for next {TEST_DAYS} days:\")\n",
    "print(future_df.to_string())\n",
    "\n",
    "# Plot future forecast\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "hist = daily_data[RESPONSE_VARIABLE].iloc[-60:]\n",
    "ax.plot(hist.index, hist.values, label='Historical', linewidth=2)\n",
    "ax.plot(future_df.index, future_df['predicted_price'], marker='o',\n",
    "        linestyle='--', label=f'LSTM Forecast ({TEST_DAYS}d)', color='orange')\n",
    "ax.set_title(f\"{COIN} LSTM — Next {TEST_DAYS}-Day Price Forecast\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Price (USD)\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lstm-cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Step 9: Save Predictions and Metrics ─────────────────────────────────────\n",
    "val_path = _save_validation_predictions(val_df, COIN, 'lstm')\n",
    "future_path = _save_future_predictions(future_df, COIN, 'lstm')\n",
    "metrics_path = _save_metrics(std_rmse, COIN, 'lstm')\n",
    "\n",
    "print(f\"Validation predictions saved to: {val_path}\")\n",
    "print(f\"Future predictions saved to:     {future_path}\")\n",
    "print(f\"Metrics (std RMSE) saved to:     {metrics_path}\")\n",
    "print(f\"\\nStandardized RMSE: {std_rmse:.4f}\")\n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}