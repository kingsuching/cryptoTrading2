{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transformer-cell-01",
   "metadata": {},
   "outputs": [],
   "source": "import os, sys\nif os.path.basename(os.getcwd()) == 'notebooks':\n    os.chdir('..')\nif os.getcwd() not in sys.path:\n    sys.path.insert(0, os.getcwd())\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom files.functions import (\n    fullDataPath,\n    dataSetup,\n    trainingCols,\n    normalize_data,\n    create_sequences,\n    _get_device,\n    _transformer_param_grid,\n    _torch_train,\n    _seq_future_forecast_torch,\n    _standardized_rmse,\n    _save_model_artifact,\n    _save_validation_predictions,\n    _save_future_predictions,\n    _save_metrics,\n)\nfrom files.CONSTANTS import COIN, RESPONSE_VARIABLE, TRAINING_COLUMNS, TEST_DAYS, TRAIN_PCT\nfrom implementations.transformer_model import CryptoTransformer"
  },
  {
   "cell_type": "markdown",
   "id": "transformer-cell-02",
   "metadata": {},
   "source": [
    "# Transformer Training\n",
    "\n",
    "CryptoTransformer — a Transformer encoder with sinusoidal positional encoding and multi-head\n",
    "self-attention for multivariate crypto price prediction using PyTorch.\n",
    "This notebook tunes embedding dimension (`d_model`), number of attention heads (`nhead`),\n",
    "encoder depth, feedforward dimension, and dropout. Only configurations where\n",
    "`d_model % nhead == 0` are valid and included in the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transformer-cell-03",
   "metadata": {},
   "outputs": [],
   "source": "# ── Step 1: Load and Prepare Data ────────────────────────────────────────────\nSEQ_LEN = 30\nSCALER_METHOD = 'minmax'  # default scaler\n\nraw_path = fullDataPath(COIN)\ndata = pd.read_csv(raw_path)\ndaily_data = dataSetup(data, trainingColPath=TRAINING_COLUMNS, response=RESPONSE_VARIABLE)\ncols = trainingCols(TRAINING_COLUMNS)\ndata_full = daily_data[cols + [RESPONSE_VARIABLE]].copy()\nprint(f\"Dataset shape: {daily_data.shape}\")\nprint(f\"Features ({len(cols)}): {cols}\")\nprint(f\"Date range: {daily_data.index[0]} → {daily_data.index[-1]}\")\nprint(f\"\\ndata_full shape: {data_full.shape}  (features + target)\")\n\ndaily_data[[RESPONSE_VARIABLE]].plot(title=f\"{COIN} Close Price\", figsize=(12, 4))\nplt.tight_layout()\nplt.show()\n\n# Normalize with default scaler\nscaled_data, feat_sc, tgt_sc = normalize_data(\n    data_full, method=SCALER_METHOD, target_col=RESPONSE_VARIABLE\n)\n\n# ── Step 2: Create Sequences and Train/Val Split ──────────────────────────────\nX_seq, y_seq = create_sequences(scaled_data, sequence_length=SEQ_LEN, prediction_horizon=1)\nn_train = max(1, int(len(X_seq) * TRAIN_PCT))\n\nX_tr = torch.tensor(X_seq[:n_train], dtype=torch.float32)\ny_tr = torch.tensor(y_seq[:n_train].flatten(), dtype=torch.float32)\nX_va = torch.tensor(X_seq[n_train:], dtype=torch.float32)\ny_va_np = y_seq[n_train:].flatten()\ny_va_t = torch.tensor(y_va_np, dtype=torch.float32)\n\nnum_features = X_tr.shape[2]\ndevice = _get_device()\nprint(f\"\\nSequence length: {SEQ_LEN}\")\nprint(f\"Train sequences: {len(X_tr)}  |  Val sequences: {len(X_va)}\")\nprint(f\"Input features: {num_features}\")\nprint(f\"Device: {device}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transformer-cell-04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Step 3: Define Hyperparameter Grid ───────────────────────────────────────\n",
    "# Only keep configs where d_model % nhead == 0 (required by MultiheadAttention)\n",
    "full_grid = _transformer_param_grid()\n",
    "param_grid = [cfg for cfg in full_grid if cfg['d_model'] % cfg['nhead'] == 0]\n",
    "\n",
    "print(f\"Full grid size:   {len(full_grid)} configs\")\n",
    "print(f\"Valid grid size:  {len(param_grid)} configs  (d_model % nhead == 0)\")\n",
    "print(\"\\nValid configs:\")\n",
    "for cfg in param_grid:\n",
    "    print(\" \", cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transformer-cell-05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Step 4: Tune Hyperparameters ─────────────────────────────────────────────\n",
    "best_score = np.inf\n",
    "best_combo = None\n",
    "tuning_results = []\n",
    "\n",
    "print(\"Running Transformer hyperparameter search...\")\n",
    "for params in param_grid:\n",
    "    # Guard: skip if d_model % nhead != 0 (should already be filtered)\n",
    "    if params['d_model'] % params['nhead'] != 0:\n",
    "        print(f\"  SKIPPED (d_model={params['d_model']} % nhead={params['nhead']} != 0): {params}\")\n",
    "        continue\n",
    "    try:\n",
    "        m = CryptoTransformer(num_features=num_features, **params)\n",
    "        m, val_loss = _torch_train(\n",
    "            m, X_tr, y_tr, X_va, y_va_t,\n",
    "            lr=1e-3, epochs=30, batch_size=32, patience=5, device=device\n",
    "        )\n",
    "        rec = {**params, 'val_loss': val_loss}\n",
    "        tuning_results.append(rec)\n",
    "        if val_loss < best_score:\n",
    "            best_score = val_loss\n",
    "            best_combo = rec.copy()\n",
    "        print(f\"  d_model={params['d_model']} nhead={params['nhead']} \"\n",
    "              f\"layers={params['num_encoder_layers']} ff={params['dim_feedforward']}  \"\n",
    "              f\"val_loss={val_loss:.6f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR {params}: {e}\")\n",
    "\n",
    "if best_combo is None:\n",
    "    best_combo = {'d_model': 64, 'nhead': 4, 'num_encoder_layers': 2,\n",
    "                  'dim_feedforward': 256, 'dropout': 0.1, 'val_loss': np.nan}\n",
    "    print(\"WARNING: No valid config found; using defaults.\")\n",
    "\n",
    "print(f\"\\nBest combo: {best_combo}\")\n",
    "\n",
    "tuning_df = pd.DataFrame(tuning_results).sort_values('val_loss').reset_index(drop=True)\n",
    "print(\"\\nTop-5 tuning results:\")\n",
    "print(tuning_df.head(5).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transformer-cell-06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Step 5: Train Best Model ──────────────────────────────────────────────────\n",
    "best_hp = {k: v for k, v in best_combo.items() if k != 'val_loss'}\n",
    "print(f\"Training final Transformer with: {best_hp}\")\n",
    "\n",
    "best_model = CryptoTransformer(num_features=num_features, **best_hp)\n",
    "best_model, final_val_loss = _torch_train(\n",
    "    best_model, X_tr, y_tr, X_va, y_va_t,\n",
    "    lr=1e-3, epochs=50, batch_size=32, patience=10, device=device\n",
    ")\n",
    "best_model.eval()\n",
    "print(f\"Final val loss: {final_val_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transformer-cell-07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Step 6: Save Model to models/{COIN}/ ─────────────────────────────────────\n",
    "os.makedirs(f'models/{COIN}', exist_ok=True)\n",
    "model_pt_path = f'models/{COIN}/{COIN}_transformer_model.pt'\n",
    "torch.save(best_model.state_dict(), model_pt_path)\n",
    "print(f\"Model state dict saved to: {model_pt_path}\")\n",
    "\n",
    "meta_path = _save_model_artifact(\n",
    "    {'feat_scaler': feat_sc, 'tgt_scaler': tgt_sc, 'params': best_combo,\n",
    "     'seq_len': SEQ_LEN, 'scaler_method': SCALER_METHOD},\n",
    "    COIN,\n",
    "    f'{COIN}_transformer_meta.pkl'\n",
    ")\n",
    "print(f\"Meta artifact saved to: {meta_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transformer-cell-08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Step 7: Predict on Validation Set ────────────────────────────────────────\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    val_preds_scaled = best_model(X_va).squeeze(-1).numpy()\n",
    "\n",
    "# Inverse-transform to price scale\n",
    "val_preds = tgt_sc.inverse_transform(val_preds_scaled.reshape(-1, 1)).flatten()\n",
    "val_true = tgt_sc.inverse_transform(y_va_np.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Align with daily_data index\n",
    "val_idx = daily_data.index[n_train + SEQ_LEN: n_train + SEQ_LEN + len(val_preds)]\n",
    "val_df = pd.DataFrame({'predicted_price': val_preds}, index=val_idx[:len(val_preds)])\n",
    "\n",
    "# Compute RMSE\n",
    "rmse = float(np.sqrt(np.mean((val_true - val_preds) ** 2)))\n",
    "std_rmse = rmse / float(np.std(val_true)) if float(np.std(val_true)) > 0 else rmse\n",
    "print(f\"Validation RMSE:            {rmse:,.2f}\")\n",
    "print(f\"Standardized RMSE (÷ std):  {std_rmse:.4f}\")\n",
    "\n",
    "# Plot actual vs predicted\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "ax.plot(val_idx[:len(val_true)], val_true, label='Actual', linewidth=2)\n",
    "ax.plot(val_df.index, val_df['predicted_price'], label='Predicted (Transformer)', linestyle='--')\n",
    "ax.set_title(f\"{COIN} Transformer — Validation Set: Actual vs Predicted\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Price (USD)\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transformer-cell-09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Step 8: Predict Next TEST_DAYS Days ──────────────────────────────────────\n",
    "future_df = _seq_future_forecast_torch(\n",
    "    best_model, scaled_data, SEQ_LEN, tgt_sc, daily_data, n=TEST_DAYS\n",
    ")\n",
    "\n",
    "print(f\"Future predictions for next {TEST_DAYS} days:\")\n",
    "print(future_df.to_string())\n",
    "\n",
    "# Plot future forecast\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "hist = daily_data[RESPONSE_VARIABLE].iloc[-60:]\n",
    "ax.plot(hist.index, hist.values, label='Historical', linewidth=2)\n",
    "ax.plot(future_df.index, future_df['predicted_price'], marker='o',\n",
    "        linestyle='--', label=f'Transformer Forecast ({TEST_DAYS}d)', color='orange')\n",
    "ax.set_title(f\"{COIN} Transformer — Next {TEST_DAYS}-Day Price Forecast\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Price (USD)\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transformer-cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Step 9: Save Predictions and Metrics ─────────────────────────────────────\n",
    "val_path = _save_validation_predictions(val_df, COIN, 'transformer')\n",
    "future_path = _save_future_predictions(future_df, COIN, 'transformer')\n",
    "metrics_path = _save_metrics(std_rmse, COIN, 'transformer')\n",
    "\n",
    "print(f\"Validation predictions saved to: {val_path}\")\n",
    "print(f\"Future predictions saved to:     {future_path}\")\n",
    "print(f\"Metrics (std RMSE) saved to:     {metrics_path}\")\n",
    "print(f\"\\nStandardized RMSE: {std_rmse:.4f}\")\n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}